import itertools
import csv
import time
import os
import sys

from typing import List
import numpy as np
from multiprocessing import Pool, Manager

from .losses import *
from .regularizers import *


keys_training_params = ["epochs", "batch_size", "early_stopping"] 
keys_training_params_direct = ["lambda_", "p_d", "p_dc"]


def save_results_to_file(keys, gs_results, file_name):
    """
    A function used by grid search functions to save the results in a csv file whose
    file_name is indicated by parameter.

    Args:
        keys: (list) list of the parameter names
        gs_results: (list) list of results (tuples: params, loss_tr, loss_vl) given by grid search functions
        file_name: (str) name of the file that will be saved in the execution directory
    """
    timestr = time.strftime("%Y-%m-%d-%H%M%S")
    path = os.path.splitext(file_name)[0]

    with open(path+"-"+timestr+".csv", 'w') as f:
        writer = csv.writer(f)

        writer.writerow(keys + ["loss_tr", "loss_vl"])

        for (params, loss_tr, loss_vl) in gs_results:
            writer.writerow(list(params.values()) + [loss_tr, loss_vl])


def grid_parallel(shared_queue, model, training_data, valid_data, direct, training_params, search_param):
    """
    Function called by the processes spawned by multiprocessing functions

    Args:
        shared_queue: (Manager.Queue) locked queue to store results in a async mode by multiple processes
        model: the model
        training_data: (tuple) the training_x and training_y
        valid_data: (tuple) the valid_x and valid_y
        direct: (boolean) flag that indicates if the model's training phase is direct or not
        training_params: (dict) parameters to pass to the training method of the model
        search_param: (dict) parameters of the current iteration of the grid_search
    """
    if not direct:
        history = model.training(training_data, valid_data, **training_params)
        shared_queue.put((search_param, history["loss_tr"][-1], history["loss_vl"][-1]))
    else:
        history = model.direct_training(training_data, valid_data, **training_params)
        shared_queue.put((search_param, history["loss_tr"], history["loss_vl"]))
    print(search_param, "  : done!!")


def grid_parallel_cv(shared_queue, build_model, dataset, k_folds, direct, static_params, search_param):
    """
    Function called by the processes spawned by multiprocessing functions

    Args:
        shared_queue: (Manager.Queue) locked queue to store results in a async mode by multiple processes
        build_model: method to build the model
        dataset: (tuple) the x and y
        k_folds: (int) number of folds of the cross validation
        direct: (boolean) flag that indicates if the model's training phase is direct or not
        static_params: (dict) parameters not object of the grid search process
        search_param: (dict) parameters of the current iteration of the grid_search
    """
    loss_tr_cv, loss_vl_cv = cross_validation(build_model, dataset, {**static_params, **search_param}, k_folds, direct)
    shared_queue.put((search_param, loss_tr_cv, loss_vl_cv))
    print(search_param, " : done!!")


def best_comb(gs_results):
    """
    A function called by grid search functions that return best combination of parameters
    given the loss over validation dataset.

    Args:
        gs_results: (list) list of tuple (search_param, loss_tr, loss_vl) generated by grid search functions
    Returns:
        best_combination: (dict) returns the best combination of the searching parameters
    """
    best_result = np.inf
    best_combination = None
    for (search_param, loss_tr, loss_vl) in gs_results:

        print(f"{search_param} -> tr: {loss_tr} | vl: {loss_vl}")

        if best_result > loss_vl:
           best_result = loss_vl
           best_combination = search_param

    return best_combination


def split_train_params(params, direct=False):
    """
    Splitting parameters given in input with training and others

    Args:
        params: (dict) dictionary of parameters to split
        direct: (boolean) flag that indicates if the training is referred to direct or undirect method
    Returns:
        other_params: (dict) non training parameters
        training_params: (dict) training parameters
    """

    if not direct:
        keys_training = keys_training_params
    else:
        keys_training = keys_training_params_direct

    other_params = { key:value for key,value in params.items() if key not in keys_training}
    training_params = { key:value for key,value in params.items() if key in keys_training}
    return other_params, training_params


def split_search_params(params):
    """
    A function that splits search parameters with respect to static parameters
    search parameters are the one whose type is list.

    Args:
        params: (dict) parameters to split
    Returns:
        static_params: (dict) static parameters
        search_params: (dict) searching parameters
    """
    static_params = { key:value for key,value in params.items() if not isinstance(value, List) }
    search_params = { key:value for key,value in params.items() if isinstance(value, List)}
    return static_params, search_params


def cross_validation(build_model, dataset, params, k_folds=4, direct=False):
    """
    Perform a k-fold cross-validation

    Args:
        build_model: function that return the compiled model
        dataset: (tuple) dataset X and y
        params: (dict) dictionary of the parameters to pass to the build_model and it's training phase
        k_folds: (int) number of folds of cross validation
        direct: (boolean) flag to indicate if the training phase of the model is directo or not
    Returns:
        loss_tr_mean: (double) the error mean of the cv on the training dataset
        loss_vl_mean: (double) the error mean of the cv on the validation dataset
    """
    X, y = dataset

    l = len(X)
    l_vl = l//k_folds

    build_params, train_params = split_train_params(params, direct)

    loss_tr_mean = 0
    loss_vl_mean = 0

    for k in range(k_folds):
        # cicle over folds, for every fold create train_x, valid_x, train_y, valid_y
        if k != k_folds-1: # exclude the k-th part from the validation
            train_x = np.append(X[:(k)*l_vl], X[(k+1)*l_vl:], axis=0)
            train_y = np.append(y[:(k)*l_vl], y[(k+1)*l_vl:], axis=0)
            valid_x = X[k*l_vl:(k+1)*l_vl]
            valid_y = y[k*l_vl:(k+1)*l_vl]
        else: # last fold clausole
            train_x = X[:k*l_vl]
            train_y = y[:k*l_vl]
            valid_x = X[k*l_vl:]
            valid_y = y[k*l_vl:]

        model = build_model(**build_params)

        if not direct:
            history = model.training((train_x, train_y), (valid_x, valid_y), **train_params)
            loss_tr_mean += history["loss_tr"][-1]
            loss_vl_mean += history["loss_vl"][-1]
        else:
            history = model.direct_training((train_x, train_y), (valid_x, valid_y), **train_params)
            loss_tr_mean += history["loss_tr"]
            loss_vl_mean += history["loss_vl"]

    loss_tr_mean /= k_folds
    loss_vl_mean /= k_folds

    return loss_tr_mean, loss_vl_mean


def grid_search_cv(build_model, dataset, params, k_folds=4, direct=False, path=None):
    """
    Perform a grid search in which for every n-uple of parameters we use a k_fold cross validation
    for a better estimate of training and validation error.
    
    Args:
        build_model: function that return a compiled model
        dataset: (tuple) dataset X and y
        params: (dict) dictionary of parameters to pass to the build_model function
        k_folds: (int) number of folds of cross validation
        direct: (boolean) flag to indicate if the training phase of the model is directo or not
        path: (str) file name used to save the grid_search results in a csv format
    Returns:
        return_data: (dict) the best_combination of parameters
    """
    static_params, search_params = split_search_params(params)
    m = Manager()
    shared_queue = m.Queue()

    pool = Pool() # use all available cores, otherwise specify the number you want as an argument
    
    try:

        for param_combination in itertools.product(*search_params.values()):
            # create dictionary for params
            search_param = {}
            for i, param_key in enumerate(search_params.keys()):
                search_param[param_key] = param_combination[i]

            print("-> ", search_param)

            # here i have data to pass to the workers
            pool.apply_async(grid_parallel_cv, (shared_queue, build_model, dataset, k_folds, direct, static_params, search_param))
        
        pool.close()
        pool.join()

    except KeyboardInterrupt:
        pool.terminate()
        sys.exit(1)

    gs_results = []
    while not shared_queue.empty():
        gs_results.append(shared_queue.get())

    # salvare i risultati della grid search in un file se presente il path
    if path:
        save_results_to_file(list(search_params.keys()), gs_results, path)

    return_data = {**best_comb(gs_results), **static_params}

    return return_data


# drop to the build_model the task to assign the params to build the model
def grid_search(build_model, training_data, valid_data, params, direct=False, path=None):
    """
    Perform a classic grid_search.

    Args:
        build_model: function that return a compiled model
        training_data : (tuple) dataset training_X and training_y
        valid_data : (tuple) dataset valid_X and valid_y
        params: (dict) dictionary of parameters to pass to the build_model function
        direct: (boolean) flag to indicate if the training phase of the model is directo or not
        path: (str) file name used to save the grid_search results in a csv format
    Returns:
        return_data: (dict) the best_combination of parameters
    """

    static_params, search_params = split_search_params(params)
    m = Manager()
    shared_queue = m.Queue()

    pool = Pool() # use all available cores, otherwise specify the number you want as an argument
    for param_combination in itertools.product(*search_params.values()):
        # create dictionary for params
        search_param = {}
        for i, param_key in enumerate(search_params.keys()):
            search_param[param_key] = param_combination[i]

        print("-> ", search_param)
                
        build_params, training_params = split_train_params({**static_params, **search_param}, direct)

        model = build_model(**build_params)

        # here i have data to pass to the workers
        pool.apply_async(grid_parallel, args=(shared_queue, model, training_data, valid_data, direct, training_params, search_param))

    pool.close()
    pool.join()

    gs_results = []
    while not shared_queue.empty():
        gs_results.append(shared_queue.get())

    # salvare i risultati della grid search in un file se presente il path
    if path:
        save_results_to_file(list(search_params.keys()), gs_results, path)


    return_data = {**best_comb(gs_results), **static_params}

    return return_data